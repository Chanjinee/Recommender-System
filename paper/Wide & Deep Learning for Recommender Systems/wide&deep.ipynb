{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631aac6c",
   "metadata": {},
   "source": [
    "## PREPARE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c02d1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "      <th>income_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education_num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital_status         occupation   relationship   race  gender  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week native_country income_bracket  \\\n",
       "0          2174             0              40  United-States          <=50K   \n",
       "1             0             0              13  United-States          <=50K   \n",
       "2             0             0              40  United-States          <=50K   \n",
       "3             0             0              40  United-States          <=50K   \n",
       "4             0             0              40           Cuba          <=50K   \n",
       "\n",
       "   income_label  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 아래의 코드를 돌릴때 나는 경고문구를 무시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DF = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# Let's create a feature that will be our target for logistic regression\n",
    "DF['income_label'] = (DF[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a03b709",
   "metadata": {},
   "source": [
    "### 1-Set the experiment\n",
    "\n",
    "We need to define the columns in the dataset that will be passed to the *\"wide-\"* and the *\"deep-side\"* of the model. For more details of what I mean by \"wide\" and \"deep\" I recommend either to read [this tutorial](https://www.tensorflow.org/tutorials/wide_and_deep), the [original paper](https://arxiv.org/pdf/1606.07792.pdf) or the demo2 in this repo. \n",
    "\n",
    "In the example below, the wide and crossed column will be passed to the wide side of the model while the embedding columns and continuous columns will go through the deep side. \n",
    "\n",
    "We also need to state our target and the method that will be used to fit/predict that target (regression, logistic or multiclass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "109d0fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide_cols와 crossed_cols는 wide로\n",
    "wide_cols = ['age','hours_per_week','education', 'relationship','workclass',\n",
    "             'occupation','native_country','gender']\n",
    "crossed_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "\n",
    "# embeddings_cols와 continuous_cols는 deep으로\n",
    "embeddings_cols = [('education',10), ('relationship',8), ('workclass',10), # column이름, 원하는 embedding size로 이루어짐\n",
    "                    ('occupation',10),('native_country',12)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n",
    "target = 'income_label'\n",
    "method = 'logistic'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5006a",
   "metadata": {},
   "source": [
    "You will see that `embeddings_cols` is a list of tuples with two elements. These are the column name and the \"dimension of the corresponding embeddings\" (i.e. the number of embeddings per feature), so that when passed through the Deep-side education will be represented by 10 embeddings, relatioship by 8, etc.\n",
    "\n",
    "If you want to use the same number of embeddings for *all* the embedding columns you can simply include the column names and define the number of embeddings when calling to the `prepare_data` function I mention before. This function has a parameter called `def_dim` (default dimension) that will be applied to all embedding columns if no embedding dimension. The first few lines on `prepare_data` look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69fe83f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_cols에 embeddings size가 없다면 default(def_dim)로 값 지정\n",
    "if type(embeddings_cols[0]) is tuple:\n",
    "    emb_dim = dict(embeddings_cols)\n",
    "    embeddings_cols = [emb[0] for emb in embeddings_cols] # 컬럼들만 따로 지정\n",
    "else:\n",
    "    emb_dim = {e:def_dim for e in embeddings_cols}\n",
    "\n",
    "# deep에 들어갈 컬럼만 따로 지정\n",
    "deep_cols = embeddings_cols+continuous_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0928a4f",
   "metadata": {},
   "source": [
    "### 2-Cross-product for binary features\n",
    "\n",
    "At explained in the original paper: *\"For binary features, a cross-product transformation (e.g.,\n",
    "`AND(gender=female, language=en))` is 1 if and only if the constituent features (`gender=female and language=en`)\n",
    "are all 1, and 0 otherwise\"*. Here, this is implemented by combining the features into a new feature and one-hot encoded it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f6eca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(DF[target])\n",
    "# We copy the original dataset so we do not mutate it -> 데이터 바뀌는거 방지를 위해 복사해놓음\n",
    "df_tmp = DF.copy()[list(set(wide_cols + deep_cols))]\n",
    "\n",
    "# Build the crossed columns\n",
    "crossed_columns = []\n",
    "for cols in crossed_cols:\n",
    "    colname = '_'.join(cols) # 컬럼명을 '_'로 합쳐줌\n",
    "    df_tmp[colname] = df_tmp[cols].apply(lambda x: '-'.join(x), axis=1) # 데이터 프레임 안에 있는 값들을 '-'로 합쳐줌\n",
    "    crossed_columns.append(colname)\n",
    "\n",
    "# Extract the categorical column names that can be one hot encoded later -> 원핫인코딩이 필요한 컬럼을 추출\n",
    "categorical_columns = list(df_tmp.select_dtypes(include=['object']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea38351",
   "metadata": {},
   "source": [
    "Let's have a look to one of the \"crossed features\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbce01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Bachelors-Adm-clerical\n",
       "1    Bachelors-Exec-managerial\n",
       "2    HS-grad-Handlers-cleaners\n",
       "3       11th-Handlers-cleaners\n",
       "4     Bachelors-Prof-specialty\n",
       "Name: education_occupation, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp['education_occupation'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814ad0a",
   "metadata": {},
   "source": [
    "나중에 원핫 인코딩 할 때, \"crossed features\"에 있는 값이 둘다 1일때만 1로 원핫인코딩됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e04a07",
   "metadata": {},
   "source": [
    "When we one-hot encode this feature later, it will be only 1 *if and only* if the two constituent features are 1. In other words, the level `Bachelors-Adm-clerical` of the `education_occupation` feature will be 1 *if and only if* for that particular observation `education=Bachelors` AND `occupation=Adm-clerical`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530abfda",
   "metadata": {},
   "source": [
    "### 3-Label-encoding and splitting the dataframe into wide and deep.\n",
    "\n",
    "We first encode the dataframe and keep a dictionary of the encodings for those columns that will be represented as embeddings (for the remaining ones is unneccesary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a98d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df, cols=None):\n",
    "    \"\"\"\n",
    "    Helper function to label-encode some features of a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    --------\n",
    "    df  (pd.Dataframe)\n",
    "    cols (list): optional - columns to be label-encoded\n",
    "\n",
    "    Returns:\n",
    "    ________\n",
    "    val_to_idx (dict) : Dictionary of dictionaries with useful information about\n",
    "    the encoding mapping\n",
    "    df (pd.Dataframe): mutated df with Label-encoded features.\n",
    "    \"\"\"\n",
    "\n",
    "    if cols == None:\n",
    "        cols = list(df.select_dtypes(include=['object']).columns) # 카테고리컬 변수만 추출\n",
    "\n",
    "    val_types = dict()\n",
    "    for c in cols:\n",
    "        val_types[c] = df[c].unique() # val_types에 카테고리컬 변수의 갯수를 저장\n",
    "\n",
    "    val_to_idx = dict()\n",
    "    for k, v in zip(val_types.keys(),val_types.values()):\n",
    "        val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}\n",
    "\n",
    "    for k, v in zip(val_to_idx.keys(),val_to_idx.values()):\n",
    "        df[k] = df[k].apply(lambda x: v[x])\n",
    "\n",
    "    return val_to_idx, df\n",
    "\n",
    "# deep cols 임베딩 해줌\n",
    "# Encode the dataframe and get the encoding Dictionary only for the\n",
    "# deep_cols (for the wide_cols is uneccessary)\n",
    "encoding_dict,df_tmp = label_encode(df_tmp)\n",
    "encoding_dict = {k:encoding_dict[k] for k in encoding_dict if k in deep_cols}\n",
    "embeddings_input = []\n",
    "for k,v in zip(encoding_dict.keys(),encoding_dict.values()):\n",
    "    embeddings_input.append((k, len(v), emb_dim[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984ce32",
   "metadata": {},
   "source": [
    "Then we split the data frame into the wide and deep data frames and keep the index of the deep column. This information will be used later since we will slice the tensors based on index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e2433",
   "metadata": {},
   "source": [
    "df_deep과 de_wide로 나뉨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740abc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## de_deep ##\n",
    "# select the deep_cols and get the column index that will be use later\n",
    "# to slice the tensors\n",
    "df_deep = df_tmp[deep_cols]\n",
    "deep_column_idx = {k:v for v,k in enumerate(df_deep.columns)} # 나중에 텐서를 자를 때, 사용하기 때문에 저장해줌\n",
    "\n",
    "# The continous columns will be concatenated with the embeddings, so you\n",
    "# might want to normalize them first\n",
    "# -> 연속형 변수는 임베딩 벡터들과 concat됨. 먼저 정규화 하는 작업을 거쳐도됨\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "for cc in continuous_cols:\n",
    "    df_deep[cc]  = scaler.fit_transform(df_deep[cc].values.reshape(-1,1)) # 열을 1로 맞춰줌\n",
    "\n",
    "## df_wide ##\n",
    "# wide에 들어갈 데이터 생성\n",
    "df_wide = df_tmp[wide_cols+crossed_columns]\n",
    "del(df_tmp)\n",
    "\n",
    "# categorical 변수는 원핫 인코딩 해줌\n",
    "dummy_cols = [c for c in wide_cols+crossed_columns if c in categorical_columns]\n",
    "df_wide = pd.get_dummies(df_wide, columns=dummy_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cbaf3c",
   "metadata": {},
   "source": [
    "### 4-Train/Test split and build the output dictionary\n",
    "\n",
    "I think the code here is self explanatory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12b6426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple\n",
    "\n",
    "# 검증을 위해 데이터를 나눔\n",
    "seed = 1981\n",
    "X_train_deep, X_test_deep = train_test_split(df_deep.values, test_size=0.3, random_state=seed)\n",
    "X_train_wide, X_test_wide = train_test_split(df_wide.values, test_size=0.3, random_state=seed)\n",
    "y_train, y_test = train_test_split(Y, test_size=0.3, random_state=1981)\n",
    "\n",
    "# Building the output dictionary\n",
    "wd_dataset = dict()\n",
    "train_dataset = namedtuple('train_dataset', 'wide, deep, labels')\n",
    "test_dataset  = namedtuple('test_dataset' , 'wide, deep, labels')\n",
    "wd_dataset['train_dataset'] = train_dataset(X_train_wide, X_train_deep, y_train)\n",
    "wd_dataset['test_dataset']  = test_dataset(X_test_wide, X_test_deep, y_test)\n",
    "wd_dataset['embeddings_input']  = embeddings_input\n",
    "wd_dataset['deep_column_idx'] = deep_column_idx\n",
    "wd_dataset['encoding_dict'] = encoding_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35da855",
   "metadata": {},
   "source": [
    "`wd_dataset` is a dictionary with all the neccesary information. Let's have a look to for example, the `train_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f4cec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_dataset(wide=array([[46, 50,  0, ...,  0,  0,  0],\n",
       "       [32, 45,  1, ...,  0,  0,  0],\n",
       "       [30, 30,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [40, 40,  0, ...,  0,  0,  0],\n",
       "       [45, 37,  1, ...,  0,  0,  0],\n",
       "       [40, 45,  1, ...,  0,  0,  0]], dtype=int64), deep=array([[ 3.        ,  1.        ,  6.        , ...,  0.        ,\n",
       "         0.53655844,  0.77292975],\n",
       "       [ 0.        ,  0.        ,  2.        , ...,  0.        ,\n",
       "        -0.48456647,  0.36942139],\n",
       "       [ 1.        ,  4.        ,  2.        , ...,  0.        ,\n",
       "        -0.63044146, -0.84110367],\n",
       "       ...,\n",
       "       [ 1.        ,  0.        ,  2.        , ...,  0.        ,\n",
       "         0.09893348, -0.03408696],\n",
       "       [ 0.        ,  1.        ,  2.        , ...,  0.        ,\n",
       "         0.46362095, -0.27619198],\n",
       "       [ 0.        ,  1.        ,  2.        , ...,  0.        ,\n",
       "         0.09893348,  0.36942139]]), labels=array([1, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_dataset['train_dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3553e65",
   "metadata": {},
   "source": [
    "### 2-The model\n",
    "\n",
    "The model is a combination of a linear classifier/regressor for sparse features (Wide) plus a neural network classifier/regressor that receives the embeddings. The figure below, taken from the [tutorial](https://www.tensorflow.org/tutorials/wide_and_deep), is a good illustration on how the algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa08b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# PATH = \"/Users/javier/Desktop/wide_deep.png\"\n",
    "# Image(filename = PATH, width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed647b",
   "metadata": {},
   "source": [
    "A priori, \"all\" we have to do is:\n",
    "\n",
    "1. Prepare the wide part\n",
    "\n",
    "2. Prepare the deep part\n",
    "\n",
    "3. combine them\n",
    "\n",
    "So...let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c09f1",
   "metadata": {},
   "source": [
    "### 2_1. The Wide Part\n",
    "\n",
    "The wide part consist simply in the sparse features connected directly to the output neuron (or neurons if the problem is a multiclass classification). In the example here we will perform a logistic regression, so we need to connect the input features to an output neuron and use a *Sigmoid* activation function. \n",
    "\n",
    "In our case, this could be done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af47a5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=798, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class  = 1\n",
    "wide_part = nn.Linear(wide_dim, n_class)\n",
    "\n",
    "print(wide_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1844968",
   "metadata": {},
   "source": [
    "Of course, we want our code to look pretty and be functional. When using pytorch, models are normally defined as classes (although if the are simple enough one could use the `Sequential` API) that inherit the methods from the `nn` module. Let's define the wide part properly and see how to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77678227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Wide(nn.Module):\n",
    "    \"\"\"\n",
    "    Wide-side consists in simply in \"pluging\" the features into the output neuron(s)\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    wide_dim: int. Number of features per observation\n",
    "    method  : str. Regression, logistic or multiclass\n",
    "    n_class : int. number of classes. Defaults to 1 if logistic or regression\n",
    "    \"\"\"\n",
    "    def __init__(self, wide_dim, n_class):\n",
    "\n",
    "        super(Wide, self).__init__()\n",
    "        self.wide_dim = wide_dim\n",
    "        self.n_class = n_class\n",
    "\n",
    "        self.linear = nn.Linear(self.wide_dim, self.n_class)\n",
    "\n",
    "    def forward(self,X):\n",
    "\n",
    "        out = F.sigmoid(self.linear(X))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c04a7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class  = 1\n",
    "wide_model = Wide(wide_dim, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32ed5049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide(\n",
      "  (linear): Linear(in_features=798, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(wide_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c17ec",
   "metadata": {},
   "source": [
    "Ok, so far so good. We have simply created a model that consists of observations of 798 sparse features \"plugged\" into an output neuron that is activated with a *Sigmoid* function. Now, all we need to do is prepare the tensor to be passed through the model. Remember that the `prepare-data` function returns a dictionary with the wide and deep part arrays. Therefore, we can use the `train_dataset` to build the input tensor. \n",
    "\n",
    "Here the first column will be the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "792688b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34189, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_dataset['train_dataset'].labels.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ecc83bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 46, 50, ...,  0,  0,  0],\n",
       "       [ 0, 32, 45, ...,  0,  0,  0],\n",
       "       [ 0, 30, 30, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0, 40, 40, ...,  0,  0,  0],\n",
       "       [ 0, 45, 37, ...,  0,  0,  0],\n",
       "       [ 0, 40, 45, ...,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.hstack: 두 배열을 왼쪽에서 오른쪽으로 붙이기 \n",
    "train_dataset = np.hstack([wd_dataset['train_dataset'].labels.reshape(-1, 1), wd_dataset['train_dataset'].wide])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312be3a9",
   "metadata": {},
   "source": [
    "Now we need to set up the training: optimizer, loss function, etc.\n",
    "\n",
    "Pytorch provide a very handy functionality called `DataLoader`, at `torch.utils.data`. We will use if here to create the batches. Finally, the model needs to receive `Variables`. `Variables` support almost all operations you can perform on tensors. In addition, they define the computational graph, which will allow us later to automatically compute gradients. For more information read [here](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-variables-and-autograd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e87cd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 0.1180478036403656, accuracy: 0.8382520693790401\n",
      "Epoch 2 of 10, Loss: 0.3979654014110565, accuracy: 0.8382813185527509\n",
      "Epoch 3 of 10, Loss: 0.3123619854450226, accuracy: 0.8376085875574015\n",
      "Epoch 4 of 10, Loss: 0.35507360100746155, accuracy: 0.8375208400362689\n",
      "Epoch 5 of 10, Loss: 0.36205658316612244, accuracy: 0.8384568135950159\n",
      "Epoch 6 of 10, Loss: 0.19640900194644928, accuracy: 0.8371990991254497\n",
      "Epoch 7 of 10, Loss: 0.12821811437606812, accuracy: 0.8374038433414256\n",
      "Epoch 8 of 10, Loss: 0.30573543906211853, accuracy: 0.8377840825996665\n",
      "Epoch 9 of 10, Loss: 0.176675945520401, accuracy: 0.8377840825996665\n",
      "Epoch 10 of 10, Loss: 0.37185028195381165, accuracy: 0.8375208400362689\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(wide_model.parameters())\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "# from http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "for epoch in range(n_epochs):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        X_w = Variable(batch[:, 1:]).float()\n",
    "        y = Variable(batch[:, 0]).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = wide_model(X_w).squeeze(1)\n",
    "        loss = F.binary_cross_entropy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total+= y.size(0)\n",
    "        y_pred_cat = (y_pred > 0.5).float()\n",
    "        correct+= float((y_pred_cat == y).sum().data)\n",
    "\n",
    "    print ('Epoch {} of {}, Loss: {}, accuracy: {}'.format(epoch+1, n_epochs, (loss.data), (correct/total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4d618",
   "metadata": {},
   "source": [
    "### 2_2. Deep part\n",
    "\n",
    "Things get a bit more \"colorful\" here. \n",
    "\n",
    "Still, the deep part implemented here will be comprised by two layers of 100 and 50 neurons, so strictly speaking and under today's standards, is not very \"deep\". \n",
    "\n",
    "As mentioned earlier, the deep part receives embeddings and can also receive numerical features if one likes. The set up of the deep part is \"stored\" in our favourite dictionar `wd_dataset`. There we have two entries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53f07406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('education', 16, 10), ('workclass', 9, 10), ('native_country', 42, 12), ('occupation', 15, 10), ('relationship', 6, 8)]\n",
      "{'education': 0, 'relationship': 1, 'workclass': 2, 'occupation': 3, 'native_country': 4, 'age': 5, 'hours_per_week': 6}\n"
     ]
    }
   ],
   "source": [
    "print(wd_dataset['embeddings_input'])\n",
    "print(wd_dataset['deep_column_idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3abc51",
   "metadata": {},
   "source": [
    "These should be read as follows: the feature `workclass` has 9 unique values and it will be represented using 10 embeddings. In addition, in the input tensor to the deep part, `workclass` is at column 2. With this information, plus continuous columns list at the beginning of this notebook, we can build the deep part of the model.\n",
    "\n",
    "In pytorch, embedding layers are defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0dd24204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(16, 10)\n"
     ]
    }
   ],
   "source": [
    "col_name, unique_vals, n_emb = wd_dataset['embeddings_input'][0]\n",
    "emb_layer = nn.Embedding(unique_vals, n_emb) # 임베딩에는 파이토치에 있는 nn.embedding을 사용함\n",
    "print(emb_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13f539",
   "metadata": {},
   "source": [
    "Let's go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c52b71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep-side, which consists in a series of embeddings and numerical \n",
    "    features passed through a series of dense layers.\n",
    "\n",
    "    Params:\n",
    "    --------\n",
    "    embeddings_input (tuple): 3-elements tuple with the embeddings \"set-up\" -\n",
    "    (col_name, unique_values, embeddings dim)\n",
    "    continuous_cols (list) : list with the name of the continuum columns\n",
    "    deep_column_idx (dict) : dictionary where the keys are column names and the values\n",
    "    their corresponding index in the deep-side input tensor\n",
    "    hidden_layers (list) : list with the number of units per hidden layer\n",
    "    n_class (int) : number of classes. Defaults to 1 if logistic or regression\n",
    "    \"\"\"\n",
    "    def __init__(self,embeddings_input,continuous_cols,deep_column_idx,hidden_layers,n_class):\n",
    "\n",
    "        super(Deep, self).__init__()\n",
    "        self.deep_column_idx = deep_column_idx\n",
    "        self.embeddings_input = embeddings_input\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.n_class = n_class\n",
    "\n",
    "        # build the embeddings that will be passed through the deep side    \n",
    "        for col,val,dim in self.embeddings_input:\n",
    "            setattr(self, 'emb_layer_'+col, nn.Embedding(val, dim))\n",
    "\n",
    "        # the input dimension to the 1st hidden layer will be the sum of the\n",
    "        # embeddings dimensions plus the number of continuous features\n",
    "        input_emb_dim = np.sum([emb[2] for emb in self.embeddings_input])\n",
    "        self.linear_1 = nn.Linear(input_emb_dim+len(continuous_cols), self.hidden_layers[0])\n",
    "        \n",
    "        for i,h in enumerate(self.hidden_layers[1:],1): # enumerate 1부터 시작됨\n",
    "            setattr(self, 'linear_'+str(i+1), nn.Linear( self.hidden_layers[i-1], self.hidden_layers[i] ))\n",
    "\n",
    "        self.output = nn.Linear(self.hidden_layers[-1], n_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        emb = [getattr(self, 'emb_layer_'+col)(X[:,self.deep_column_idx[col]].long())\n",
    "               for col,_,_ in self.embeddings_input]\n",
    "\n",
    "        cont_idx = [self.deep_column_idx[col] for col in self.continuous_cols]\n",
    "        cont = [X[:, cont_idx].float()]\n",
    "\n",
    "        deep_inp = torch.cat(emb+cont, 1)\n",
    "\n",
    "        x_deep = F.relu(self.linear_1(deep_inp))\n",
    "        for i in range(1,len(self.hidden_layers)):\n",
    "            x_deep = F.relu( getattr(self, 'linear_'+str(i+1))(x_deep) )\n",
    "\n",
    "        out = F.sigmoid(self.output(x_deep))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7604c",
   "metadata": {},
   "source": [
    "Let's built the model and have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd6655eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_column_idx = wd_dataset['deep_column_idx']\n",
    "embeddings_input= wd_dataset['embeddings_input']\n",
    "hidden_layers = [100,50]\n",
    "deep_model = Deep(embeddings_input, continuous_cols, deep_column_idx, hidden_layers, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7fb3206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep(\n",
      "  (emb_layer_education): Embedding(16, 10)\n",
      "  (emb_layer_workclass): Embedding(9, 10)\n",
      "  (emb_layer_native_country): Embedding(42, 12)\n",
      "  (emb_layer_occupation): Embedding(15, 10)\n",
      "  (emb_layer_relationship): Embedding(6, 8)\n",
      "  (linear_1): Linear(in_features=52, out_features=100, bias=True)\n",
      "  (linear_2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(deep_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa04e0f",
   "metadata": {},
   "source": [
    "As we can see, the deep part is comprised by: \n",
    "\n",
    "1. 5 embedding layers of dimensions 10, 10, 10, 8 and 10 respectively\n",
    "2. The 5 embedding layers will be contactenated with the two continuous feaures (age and hours per week) so that the first hidden layer will receive tensors of dimensions (?, 10+10+10+8+10+2=50)\n",
    "3. Two hidden layers of 100 and 50 neurons\n",
    "4. The output neuron with a Sigmoid activation function\n",
    "\n",
    "All left to do now is to build the input dataset and set up the training as we did before for the wide part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a059be4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  3.        ,  1.        , ...,  0.        ,\n",
       "         0.53655844,  0.77292975],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "        -0.48456647,  0.36942139],\n",
       "       [ 0.        ,  1.        ,  4.        , ...,  0.        ,\n",
       "        -0.63044146, -0.84110367],\n",
       "       ...,\n",
       "       [ 0.        ,  1.        ,  0.        , ...,  0.        ,\n",
       "         0.09893348, -0.03408696],\n",
       "       [ 0.        ,  0.        ,  1.        , ...,  0.        ,\n",
       "         0.46362095, -0.27619198],\n",
       "       [ 0.        ,  0.        ,  1.        , ...,  0.        ,\n",
       "         0.09893348,  0.36942139]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = np.hstack([wd_dataset['train_dataset'].labels.reshape(-1, 1), wd_dataset['train_dataset'].deep])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "630f0675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 0.704898476600647, accuracy: 0.8220480271432332\n",
      "Epoch 2 of 10, Loss: 0.3213774561882019, accuracy: 0.836965105735763\n",
      "Epoch 3 of 10, Loss: 0.3190212547779083, accuracy: 0.8388955512006786\n",
      "Epoch 4 of 10, Loss: 0.2503102123737335, accuracy: 0.8404750065810641\n",
      "Epoch 5 of 10, Loss: 0.30370792746543884, accuracy: 0.8421422094825821\n",
      "Epoch 6 of 10, Loss: 0.4253805875778198, accuracy: 0.8444236450320278\n",
      "Epoch 7 of 10, Loss: 0.26143401861190796, accuracy: 0.8456521103278832\n",
      "Epoch 8 of 10, Loss: 0.29889628291130066, accuracy: 0.8458276053701483\n",
      "Epoch 9 of 10, Loss: 0.1939985156059265, accuracy: 0.8457691070227266\n",
      "Epoch 10 of 10, Loss: 0.2566056251525879, accuracy: 0.8474070607505338\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(deep_model.parameters())\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        X_d = Variable(batch[:, 1:])\n",
    "        y = Variable(batch[:, 0]).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = deep_model(X_d).squeeze(1)\n",
    "        loss = F.binary_cross_entropy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total+= y.size(0)\n",
    "        y_pred_cat = (y_pred > 0.5).float()\n",
    "        correct+= float((y_pred_cat == y).sum().data)\n",
    "\n",
    "    print ('Epoch {} of {}, Loss: {}, accuracy: {}'.format(epoch+1, n_epochs, (loss.data), (correct/total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b41ae",
   "metadata": {},
   "source": [
    "### Wide and Deep\n",
    "\n",
    "Time to combine the two parts. The code below is mostly identical to that of the `Deep` class. The two main differences are: \n",
    "\n",
    " 1. The output neuron receives now the wide and the deep side, so the input dimensions need to be adapted in both the definition of the model\n",
    " \n",
    "         self.output = nn.Linear(self.hidden_layers[-1]+self.wide_dim, n_class)\n",
    "    \n",
    "    and the forward pass  \n",
    "\n",
    "        wide_deep_input = torch.cat([x_deep, X_w], 1)\n",
    "\n",
    "\n",
    "Other than that, there are no major changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4e20fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeep(nn.Module):\n",
    "\n",
    "    def __init__(self, wide_dim, embeddings_input, continuous_cols, deep_column_idx, hidden_layers, n_class):\n",
    "\n",
    "        super(WideDeep, self).__init__()\n",
    "        self.wide_dim = wide_dim\n",
    "        self.deep_column_idx = deep_column_idx\n",
    "        self.embeddings_input = embeddings_input\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.n_class = n_class\n",
    "\n",
    "        for col,val,dim in self.embeddings_input:\n",
    "            setattr(self, 'emb_layer_'+col, nn.Embedding(val, dim))\n",
    "\n",
    "        input_emb_dim = np.sum([emb[2] for emb in self.embeddings_input])\n",
    "        self.linear_1 = nn.Linear(input_emb_dim+len(continuous_cols), self.hidden_layers[0])\n",
    "        for i,h in enumerate(self.hidden_layers[1:],1):\n",
    "            setattr(self, 'linear_'+str(i+1), nn.Linear( self.hidden_layers[i-1], self.hidden_layers[i] ))\n",
    "\n",
    "        self.output = nn.Linear(self.hidden_layers[-1]+self.wide_dim, n_class)\n",
    "\n",
    "    def forward(self, X_w, X_d):\n",
    "\n",
    "        emb = [getattr(self, 'emb_layer_'+col)(X_d[:,self.deep_column_idx[col]].long())\n",
    "               for col,_,_ in self.embeddings_input]\n",
    "\n",
    "        cont_idx = [self.deep_column_idx[col] for col in self.continuous_cols]\n",
    "        cont = [X_d[:, cont_idx].float()]\n",
    "\n",
    "        deep_inp = torch.cat(emb+cont, 1)\n",
    "\n",
    "        x_deep = F.relu(self.linear_1(deep_inp))\n",
    "        for i in range(1,len(self.hidden_layers)):\n",
    "            x_deep = F.relu( getattr(self, 'linear_'+str(i+1))(x_deep) )\n",
    "\n",
    "        wide_deep_input = torch.cat([x_deep, X_w.float()], 1)\n",
    "\n",
    "        out = F.sigmoid(self.output(wide_deep_input))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eda9f1",
   "metadata": {},
   "source": [
    "Let's build the model and have a look to what is \"inside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1193c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_deep_model = WideDeep(wide_dim, embeddings_input, continuous_cols, deep_column_idx, hidden_layers, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f747f8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep(\n",
       "  (emb_layer_education): Embedding(16, 10)\n",
       "  (emb_layer_workclass): Embedding(9, 10)\n",
       "  (emb_layer_native_country): Embedding(42, 12)\n",
       "  (emb_layer_occupation): Embedding(15, 10)\n",
       "  (emb_layer_relationship): Embedding(6, 8)\n",
       "  (linear_1): Linear(in_features=52, out_features=100, bias=True)\n",
       "  (linear_2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (output): Linear(in_features=848, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_deep_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31895359",
   "metadata": {},
   "source": [
    "As we see, everything is identical, apart from the output layer, which now receives all the features from the wide side plus the 50 dense features from the deep side. \n",
    "\n",
    "Finally, the `dataset` parameter in the `DataLoader` method needs to have  `__getitem__` and `__len__` methods itself. Here we would like to build a loader that returns, per batch, the wide and deep tensors. Therefore, I coded a simple class that will facilitate the loading and make the code more readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8fa8a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeepLoader(Dataset):\n",
    "    \"\"\"Helper to facilitate loading the data to the pytorch models.\n",
    "\n",
    "    Parameters:\n",
    "    --------\n",
    "    data: namedtuple with 3 elements - (wide_input_data, deep_inp_data, target)\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.X_wide = data.wide\n",
    "        self.X_deep = data.deep\n",
    "        self.Y = data.labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        xw = self.X_wide[idx]\n",
    "        xd = self.X_deep[idx]\n",
    "        y  = self.Y[idx]\n",
    "\n",
    "        return xw, xd, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "\n",
    "train_dataset = wd_dataset['train_dataset']\n",
    "widedeep_dataset = WideDeepLoader(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=widedeep_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a703eb",
   "metadata": {},
   "source": [
    "At this stage, we are good to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1332de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 0.14472396671772003, accuracy: 0.8244172102138114\n",
      "Epoch 2 of 10, Loss: 0.2337171882390976, accuracy: 0.8368773582146304\n",
      "Epoch 3 of 10, Loss: 0.22294625639915466, accuracy: 0.8399485214542689\n",
      "Epoch 4 of 10, Loss: 0.19736920297145844, accuracy: 0.8410014917078592\n",
      "Epoch 5 of 10, Loss: 0.4768935739994049, accuracy: 0.842727192956799\n",
      "Epoch 6 of 10, Loss: 0.3591581881046295, accuracy: 0.843282927257305\n",
      "Epoch 7 of 10, Loss: 0.10276183485984802, accuracy: 0.8431659305624616\n",
      "Epoch 8 of 10, Loss: 0.6814659833908081, accuracy: 0.8442773991634737\n",
      "Epoch 9 of 10, Loss: 0.12703849375247955, accuracy: 0.8445698909005821\n",
      "Epoch 10 of 10, Loss: 0.2659841775894165, accuracy: 0.8465588347129194\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(wide_deep_model.parameters())\n",
    "\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, (X_wide, X_deep, target) in enumerate(train_loader):\n",
    "        X_d = Variable(X_deep)\n",
    "        X_w = Variable(X_wide)\n",
    "        y = Variable(target).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = wide_deep_model(X_w, X_d).squeeze(1)\n",
    "        loss = F.binary_cross_entropy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total+= y.size(0)\n",
    "        y_pred_cat = (y_pred > 0.5).float()\n",
    "        correct+= float((y_pred_cat == y).sum().data)\n",
    "\n",
    "    print ('Epoch {} of {}, Loss: {}, accuracy: {}'.format(epoch+1, n_epochs, (loss.data), (correct/total)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6a8eb",
   "metadata": {},
   "source": [
    "And that's it.\n",
    "\n",
    "The code in this demo, with some minor rings and bells, is wrapped-up into a the class `WideDeep` at `wide_deep.torch_model` so is easy to use. \n",
    "\n",
    "If you want to see how to use it simply go to demo3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b09f025",
   "metadata": {},
   "source": [
    "출처: https://github.com/zenwan/Wide-and-Deep-PyTorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
